{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A - Baseline Model\n",
    "\n",
    "This part consists in the following tasks to be accomplished\n",
    "\n",
    "Use the Keras library to build a neural network with the following:\n",
    "\n",
    "- One hidden layer of 10 nodes, and a ReLU activation function\n",
    "\n",
    "- Use the adam optimizer and the mean squared error as the loss function.\n",
    "\n",
    "1. Randomly split the data into a training and test sets by holding 30% of the data for testing. You can use the train_test_splithelper function from Scikit-learn.\n",
    "\n",
    "2. Train the model on the training data using 50 epochs.\n",
    "\n",
    "3. Evaluate the model on the test data and compute the mean squared error between the predicted concrete strength and the actual concrete strength. You can use the mean_squared_error function from Scikit-learn.\n",
    "\n",
    "4. Repeat steps 1 - 3, 50 times, i.e., create a list of 50 mean squared errors.\n",
    "\n",
    "5. Report the mean and the standard deviation of the mean squared errors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://cocl.us/concrete_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cement                0\n",
       "Blast Furnace Slag    0\n",
       "Fly Ash               0\n",
       "Water                 0\n",
       "Superplasticizer      0\n",
       "Coarse Aggregate      0\n",
       "Fine Aggregate        0\n",
       "Age                   0\n",
       "Strength              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_data = pd.read_csv('concrete_data')\n",
    "# Check data consistency\n",
    "concrete_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen, the data is clean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the practice excercises, we can tale the predictors, which are:\n",
    "1. Cement\n",
    "2. Blast Furnace Slag\n",
    "3. Fly Ash\n",
    "4. Water\n",
    "5. Superplasticizer\n",
    "6. Coarse Aggregate\n",
    "7. Fine Aggregate\n",
    "\n",
    "The variable we would like to predict is\n",
    "1. **Concrete strength** given the variables mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['Cement', 'Blast Furnace Slag', 'Fly Ash', 'Water', 'Superplasticizer',\n",
    "              'Coarse Aggregate', 'Fine Aggregate']\n",
    "# The number of predictors will be used to create the inputs of the \n",
    "# neural network\n",
    "n_cols = len(predictors)\n",
    "X = concrete_data[predictors]\n",
    "y = concrete_data['Strength']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define regression model\n",
    "def baseline_model():\n",
    "    # create model with only one hidden layer with 10 nodes\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = baseline_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of the 50 mean square errors is: 163.73687766129467\n",
      "The standard deviation of the 50 mean square errors is: 5.950482684761008\n"
     ]
    }
   ],
   "source": [
    "mean_sqr_err = list()\n",
    "for i in range(50):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.30, random_state=42)\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, verbose=0)\n",
    "    y_hat = model.predict(X_test)\n",
    "    msqr_err = mean_squared_error(y_test, y_hat)\n",
    "    mean_sqr_err.append(msqr_err)\n",
    "    \n",
    "print(\"The mean of the 50 mean square errors is: %.3f\" % np.mean(mean_sqr_err))\n",
    "print(\"The standard deviation of the 50 mean square errors is: %.3f\" % np.std(mean_sqr_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B - Normalize the data\n",
    "Repeat Part A but use a normalized version of the data. Recall that one way to normalize the data is by subtracting the mean from the individual predictors and dividing by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = (X - X.mean()) / X.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of the 50 mean square errors is: 167.629\n",
      "The standard deviation of the 50 mean square errors is: 1.250\n"
     ]
    }
   ],
   "source": [
    "mean_sqr_err_norm = list()\n",
    "for i in range(50):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_norm, y, test_size=0.30, random_state=42)\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, verbose=0)\n",
    "    y_hat = model.predict(X_test)\n",
    "    msqr_err = mean_squared_error(y_test, y_hat)\n",
    "    mean_sqr_err_norm.append(msqr_err)\n",
    "    \n",
    "print(\"The mean of the 50 mean square errors is: %.3f\" % np.mean(mean_sqr_err_norm))\n",
    "print(\"The standard deviation of the 50 mean square errors is: %.3f\" % np.std(mean_sqr_err_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the mean of the mean squared errors compare to that from Step A?\n",
    "\n",
    "The mean of the mean squared errors is slightly higher. However, normalized data shows a significantly lower standard deviation, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C - More Epochs\n",
    "Repeat Part B but use 100 epochs this time for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of the 100 mean square errors is: 171.616\n",
      "The standard deviation of the 100 mean square errors is: 1.622\n"
     ]
    }
   ],
   "source": [
    "mean_sqr_err_norm = list()\n",
    "for i in range(50):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_norm, y, test_size=0.30, random_state=42)\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0)\n",
    "    y_hat = model.predict(X_test)\n",
    "    msqr_err = mean_squared_error(y_test, y_hat)\n",
    "    mean_sqr_err_norm.append(msqr_err)\n",
    "    \n",
    "print(\"The mean of the 100 mean square errors is: %.3f\" % np.mean(mean_sqr_err_norm))\n",
    "print(\"The standard deviation of the 100 mean square errors is: %.3f\" % np.std(mean_sqr_err_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the mean of the mean squared errors compare to that from Step B?\n",
    "\n",
    "In this case the mean of the mean squared errors is a bit higher, increasing the epochs doesn't improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D - More hidden layers\n",
    "Repeat part B but use a neural network with the following instead:\n",
    "\n",
    "- Three hidden layers, each of 10 nodes and ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deeper_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "model = deeper_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of the 50 mean square errors is: 159.690\n",
      "The standard deviation of the 50 mean square errors is: 5.265\n"
     ]
    }
   ],
   "source": [
    "mean_sqr_err_norm = list()\n",
    "for i in range(50):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_norm, y, test_size=0.30, random_state=42)\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, verbose=0)\n",
    "    y_hat = model.predict(X_test)\n",
    "    msqr_err = mean_squared_error(y_test, y_hat)\n",
    "    mean_sqr_err_norm.append(msqr_err)\n",
    "    \n",
    "print(\"The mean of the 50 mean square errors is: %.3f\" % np.mean(mean_sqr_err_norm))\n",
    "print(\"The standard deviation of the 50 mean square errors is: %.3f\" % np.std(mean_sqr_err_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the mean of the mean squared errors compare to that from Step B?\n",
    "\n",
    "In the latest case the mean of the mean squared errors is lower, as a result of a deeper neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
